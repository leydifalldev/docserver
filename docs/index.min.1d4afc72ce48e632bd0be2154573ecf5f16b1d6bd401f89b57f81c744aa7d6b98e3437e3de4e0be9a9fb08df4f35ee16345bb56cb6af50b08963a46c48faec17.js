var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/infrastructure/architecture/",title:"Architecture",description:`1. Présentation #1.1. Disposition #1.2. Topologie réseaux (Global) #1.3. Topologie réseaux (Interface) #2. Webserver Pool #Inventaires: 2 intel nuc celeron Roles: Webserver Proxy Reverse-proxy Gestion de certificates Interfaces réseaux: public-network gateway-network 3. Gateway Pool #Inventaires: 5 intel nuc celeron Roles: Proxy Reverse-proxy Server DHCP NAT Gateway Kubernetes services Monitoring Interfaces réseaux: gateway-network box-network 4. Database Pool #Inventaires: 3 intel nuc i5 Roles: Servers de base de données Interfaces réseaux: box-network 5.`,content:`1. Présentation #1.1. Disposition #1.2. Topologie réseaux (Global) #1.3. Topologie réseaux (Interface) #2. Webserver Pool #Inventaires: 2 intel nuc celeron Roles: Webserver Proxy Reverse-proxy Gestion de certificates Interfaces réseaux: public-network gateway-network 3. Gateway Pool #Inventaires: 5 intel nuc celeron Roles: Proxy Reverse-proxy Server DHCP NAT Gateway Kubernetes services Monitoring Interfaces réseaux: gateway-network box-network 4. Database Pool #Inventaires: 3 intel nuc i5 Roles: Servers de base de données Interfaces réseaux: box-network 5. Search-engine Pool #Inventaires: 3 intel nuc i5 Roles: Moteur de recherche Interfaces réseaux: box-network 6. Nodes #Inventaires: 2 intel nuc celeron Roles: Server d\u0026rsquo;applications Interfaces réseaux: box-network 7. Master Pool #Inventaires: 3 intel nuc celeron Roles: Management Cluster Kubernetes Management Cluster applicatif Stockage de backup Interfaces réseaux: box-network 8. Supervisor #Inventaires: 1 intel nuc i3 Roles: Bastion Stockage de backup Containers Registry Usine logiciel Interfaces réseaux: box-network `}),e.add({id:1,href:"/docs/kubernetes/architecture/",title:"Architecture",description:`1. Présentation #Le cluster kubernetes est la partie centrale de l\u0026rsquo;infrastructure.
Dans l\u0026rsquo;infrastructure, on dispose 2 groupes d\u0026rsquo;applications:
Les applications qui joue le role de gestion du cluster Les applications qui fonctionnent au sein du cluster Kubernetes pour des usages externes 2. Inventaire #Groupes du cluster
master-pool (3 hosts) datase-pool (3 hosts) search-engine-pool (3 hosts) node-pool (2 hosts) gateway-pool (3 hosts) supervisor (1 host) 3. Architecture #4. Applications #Apiserver Scheduler Controller-manager Kube-proxy Kubelet CoreDNS Nodelocaldns Calico 5.`,content:`1. Présentation #Le cluster kubernetes est la partie centrale de l\u0026rsquo;infrastructure.
Dans l\u0026rsquo;infrastructure, on dispose 2 groupes d\u0026rsquo;applications:
Les applications qui joue le role de gestion du cluster Les applications qui fonctionnent au sein du cluster Kubernetes pour des usages externes 2. Inventaire #Groupes du cluster
master-pool (3 hosts) datase-pool (3 hosts) search-engine-pool (3 hosts) node-pool (2 hosts) gateway-pool (3 hosts) supervisor (1 host) 3. Architecture #4. Applications #Apiserver Scheduler Controller-manager Kube-proxy Kubelet CoreDNS Nodelocaldns Calico 5. Installation #L\u0026rsquo;installation du cluster est assuré par 3 roles ansibles définis dans le framework protobox
- name: Install master pool hosts: supervisor-1 become: yes roles: - role: kubernetes/kubespray-init - name: Install Kubernetes API Server LoadBalancer hosts: kube_master become: yes roles: - role: network-setup vars: netplan_init: true netplan_setup: true reboot: true - role: etc-hosts - role: keepalived - role: haproxy vars: cluster: kube_master frontend_port: \u0026quot;{{ loadbalancer_apiserver.port }}\u0026quot; backend_port: 6443 - role: reboot - name: Execute kubespray import_playbook: roles/kubernetes/kubespray/cluster.yml - name: Setup calico hosts: master-1 become: true roles: - role: kubernetes/cni - role: kubernetes/dashboard - import_playbook: playbooks-proxiserver-pool/proxiserver-reload.yml Etapes #1. Usage du role kubernetes/kubespray-init permet de cloner kubespray (module offciel ansible pour mettre en place un cluster kubernetes) et l\u0026rsquo;intégrer également dans le framework protobox
2. Installation du Load-balancer pour s\u0026rsquo;interfacer avec apiserver Le control plane est répliqué sur 3 machines (master-pool) et expose l\u0026rsquo;api-server. La communication via un load-balancer répliqué sur chaque master au front de l\u0026rsquo;api. Pour cela 2 outils permettent cette configuration.
KeepAlived pour maintenir un floatingIP HAProxy pour la mise en place d\u0026rsquo;un load-balancer 3. Execution de Kubespray Kubespray se base sur l\u0026rsquo;inventory pour parametrer l\u0026rsquo;installation
-------------------------------------------------------- path: inventories/protobox/main.yml -------------------------------------------------------- ... kube_control_plane: hosts: master-1: master-2: master-3: kube_node: vars: gateway-i: 192.168.1.1 gateway-o: 192.168.0.33 hosts: gateway-1: gateway-2: etcd: hosts: master-1: master-2: master-3: k8s_cluster: children: kube_master: kube_node: calico_rr: hosts: {} ... -------------------------------------------------------- path: inventories/protobox/group_vars/all/all.yml -------------------------------------------------------- bin_dir: /usr/local/bin apiserver_loadbalancer_domain_name: \u0026quot;kube.plane.box\u0026quot; loadbalancer_apiserver: address: 192.168.1.10 port: 6442 loadbalancer_apiserver_port: 6443 loadbalancer_apiserver_healthcheck_port: 8081 no_proxy_exclude_workers: false kube_webhook_token_auth: false kube_webhook_token_auth_url_skip_tls_verify: false ntp_enabled: false ntp_manage_config: false ntp_servers: - \u0026quot;0.pool.ntp.org iburst\u0026quot; - \u0026quot;1.pool.ntp.org iburst\u0026quot; - \u0026quot;2.pool.ntp.org iburst\u0026quot; - \u0026quot;3.pool.ntp.org iburst\u0026quot; unsafe_show_logs: false ----------------------------------------------------------------- path: inventories/protobox/group_vars/k8s-cluster/k8s-cluster.yml ----------------------------------------------------------------- kube_config_dir: /etc/kubernetes kube_script_dir: \u0026quot;{{ bin_dir }}/kubernetes-scripts\u0026quot; kube_manifest_dir: \u0026quot;{{ kube_config_dir }}/manifests\u0026quot; kube_cert_dir: \u0026quot;{{ kube_config_dir }}/ssl\u0026quot; kube_token_dir: \u0026quot;{{ kube_config_dir }}/tokens\u0026quot; kube_api_anonymous_auth: true kube_version: v1.25.3 local_release_dir: \u0026quot;/tmp/releases\u0026quot; retry_stagger: 5 kube_owner: kube kube_cert_group: kube-cert kube_log_level: 2 credentials_dir: \u0026quot;{{ inventory_dir }}/credentials\u0026quot; kube_network_plugin: calico kube_network_plugin_multus: true kube_service_addresses: 10.233.0.0/18 kube_pods_subnet: 10.233.64.0/18 kube_network_node_prefix: 24 enable_dual_stack_networks: false kube_apiserver_ip: \u0026quot;{{ kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') }}\u0026quot; kube_apiserver_port: 6443 kube_proxy_mode: ipvs kube_proxy_strict_arp: true kube_proxy_nodeport_addresses: \u0026gt;- {%- if kube_proxy_nodeport_addresses_cidr is defined -%} [{{ kube_proxy_nodeport_addresses_cidr }}] {%- else -%} [] {%- endif -%} kube_encrypt_secret_data: false cluster_name: cluster.local ndots: 2 dns_mode: coredns enable_nodelocaldns: true enable_nodelocaldns_secondary: false nodelocaldns_ip: 169.254.25.10 nodelocaldns_health_port: 9254 nodelocaldns_second_health_port: 9256 nodelocaldns_bind_metrics_host_ip: false nodelocaldns_secondary_skew_seconds: 5 enable_coredns_k8s_external: false coredns_k8s_external_zone: k8s_external.local enable_coredns_k8s_endpoint_pod_names: false resolvconf_mode: host_resolvconf deploy_netchecker: false skydns_server: \u0026quot;{{ kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') }}\u0026quot; skydns_server_secondary: \u0026quot;{{ kube_service_addresses|ipaddr('net')|ipaddr(4)|ipaddr('address') }}\u0026quot; dns_domain: \u0026quot;{{ cluster_name }}\u0026quot; container_manager: containerd kata_containers_enabled: false kubeadm_certificate_key: \u0026quot;{{ lookup('password', credentials_dir + '/kubeadm_certificate_key.creds length=64 chars=hexdigits') | lower }}\u0026quot; k8s_image_pull_policy: IfNotPresent kubernetes_audit: false default_kubelet_config_dir: \u0026quot;{{ kube_config_dir }}/dynamic_kubelet_dir\u0026quot; podsecuritypolicy_enabled: false volume_cross_zone_attachment: false persistent_volumes_enabled: false event_ttl_duration: \u0026quot;1h0m0s\u0026quot; auto_renew_certificates: false kubeadm_patches: enabled: false source_dir: \u0026quot;{{ inventory_dir }}/patches\u0026quot; dest_dir: \u0026quot;{{ kube_config_dir }}/patches\u0026quot; -------------------------------------------------------- path: inventories/protobox/group_vars/all/containerd.yml -------------------------------------------------------- containerd_registry_auth: - registry: registry.protobox username: XXXXX password: XXXXX -------------------------------------------------------- path: inventories/protobox/group_vars/all/cri-o.yml -------------------------------------------------------- crio_insecure_registries: - registry.protobox crio_registry_auth: - registry: registry.protobox username: XXXXX password: XXXXX 4. Execution de kubernetes/kubectl-setup Ce module va effectuer des configurations supplémentaires telque l\u0026rsquo;intallation de kubectl, kubeadm etc
5. Execution de kubernetes/kubectl-cni Ce role est utilisé par protobox pour installer le réseau des pods de kubernetes
6. Execution de kubernetes/dashboard
6. Bilan #`}),e.add({id:2,href:"/docs/r%C3%A9seaux/networking/",title:"Architecture",description:`1. Présentation #1.1. Disposition #1.2. Topologie réseaux (Global) #1.3. Topologie réseaux (Interface) #2. Webserver Pool #Inventaires: 2 intel nuc celeron Roles: Webserver Proxy Reverse-proxy Gestion de certificates Interfaces réseaux: public-network gateway-network 3. Gateway Pool #Inventaires: 5 intel nuc celeron Roles: Proxy Reverse-proxy Server DHCP NAT Gateway Kubernetes services Monitoring Interfaces réseaux: gateway-network box-network 4. Database Pool #Inventaires: 3 intel nuc celeron Roles: Servers de base de données Interfaces réseaux: box-network 5.`,content:`1. Présentation #1.1. Disposition #1.2. Topologie réseaux (Global) #1.3. Topologie réseaux (Interface) #2. Webserver Pool #Inventaires: 2 intel nuc celeron Roles: Webserver Proxy Reverse-proxy Gestion de certificates Interfaces réseaux: public-network gateway-network 3. Gateway Pool #Inventaires: 5 intel nuc celeron Roles: Proxy Reverse-proxy Server DHCP NAT Gateway Kubernetes services Monitoring Interfaces réseaux: gateway-network box-network 4. Database Pool #Inventaires: 3 intel nuc celeron Roles: Servers de base de données Interfaces réseaux: box-network 5. Search-engine Pool #Inventaires: 3 intel nuc celeron Roles: Moteur de recherche Interfaces réseaux: box-network 6. Nodes #Inventaires: 2 intel nuc celeron Roles: Server d\u0026rsquo;applications Interfaces réseaux: box-network 7. Master Pool #Inventaires: 3 intel nuc celeron Roles: Management Cluster Kubernetes Management Cluster applicatif Stockage de backup Interfaces réseaux: box-network 8. Supervisor #Inventaires: 1 intel nuc i3 Roles: Bastion Stockage de backup Containers Registry Usine logiciel Interfaces réseaux: box-network `}),e.add({id:3,href:"/docs/factory/cycle/",title:"Cycle",description:`
1. Build Container Image #2. Push Container Image #3. Synchronize ArgoCD #4 Apply manifests to Kubernetes Cluster #`,content:`
1. Build Container Image #2. Push Container Image #3. Synchronize ArgoCD #4 Apply manifests to Kubernetes Cluster #`}),e.add({id:4,href:"/docs/authentification/oidc/",title:"OIDC",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:5,href:"/docs/authentification/pgadmin/",title:"PgAdmin4",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:6,href:"/docs/presentation/presentation/",title:"Presentation",description:`1. Introduction #Protobox est un projet on-premise cloud initié en 2020. En effet Protobox est un nano datacenter cloud permettant d\u0026rsquo;héberger des containers d\u0026rsquo;applications et orchestré par la technologie Kubernetes. Protobox est également un framework maison à base de la technologie Ansible permettant de manager totalement l\u0026rsquo;infrastructure et d\u0026rsquo;automatiser les actions souhaitées.
Elle a été conçue avec des normes de haute disponibilité et sécurité ce qui fait que son usage reste polyvalent (Environnement de preprod, test dev ou production)`,content:`1. Introduction #Protobox est un projet on-premise cloud initié en 2020. En effet Protobox est un nano datacenter cloud permettant d\u0026rsquo;héberger des containers d\u0026rsquo;applications et orchestré par la technologie Kubernetes. Protobox est également un framework maison à base de la technologie Ansible permettant de manager totalement l\u0026rsquo;infrastructure et d\u0026rsquo;automatiser les actions souhaitées.
Elle a été conçue avec des normes de haute disponibilité et sécurité ce qui fait que son usage reste polyvalent (Environnement de preprod, test dev ou production)
Elle comporte en moyenne d\u0026rsquo;une vingtaine de machines Intel NUC à consommation relativement faible. Chaque machine est taillée en fonction de son role dans le box. Ce sont des machines similaires en desing et se distinguent parfois de leurs caractéristiques.
2. Intel NUC #Les kits, mini PC et éléments Intel® NUC offrent les outils pour créer des conceptions novatrices, de la productivité d\u0026rsquo;entreprise aux solutions visuelles et aux jeux extrêmes. Conçus pour une grande gamme de charges de travail avec la qualité et la fiabilité que vous pouvez attendre d\u0026rsquo;Intel, les produits Intel® NUC permettent de développer vos offres commerciales uniques.
2.1. Contexte cloud #Dans l\u0026rsquo;infrastructure Protobox chaque intel NUC représente une unité logique du système.
2.2. principaux modèles utilisés #Dans l\u0026rsquo;infra on distingue 2 types de machine en fonction de leur processeurs. Les machines peu consommatrices de ressources sont équipées de processeur Celeron tandis que celles qui font tourner des applications lourdes en execution fonctionnent avec de Intel i5
2.3. Boitier conteneur #Ce boitier est amenagé pour contenir au maximum 18 machines et 2 écrans 13 pouces repartis en étagère plexiglass
2.4. Infrastructure #Les hosts sont placés et cloisonnés dans des sous réseaux interconnectés pour principalement des raisons de sécurité (Voir section sécurité et networking) 2.5. Déclaration et intégration d\u0026rsquo;un host dans l\u0026rsquo;infra #La déclaration d\u0026rsquo;une machine se fait dans le fichier inventory du framework protobox.
Exemple d\u0026rsquo;inventory d\u0026rsquo;un host
gateway-1: arch: x86_64 os: ubuntu_22.04 model: nuc ansible_host: 192.168.1.11 roles: - gateway networks_interfaces: box-network: dhcp: true network_manager: netplan ether: xx:xx:dd:01:4a:xx cluster-gateway: dhcp: false network_manager: netplan ip: 192.168.2.11 ether: 28:xx:xx:xx:5e:xx gateways: - to: default via: 192.168.2.254 \$ play playbook-host-install.yml `}),e.add({id:7,href:"/docs/iac/automatisation/",title:"Protodeploy",description:`1. Présentation #L\u0026rsquo;installation de l\u0026rsquo;infrastructure partant de l\u0026rsquo;adhésion des serveurs au déploiement des applications est gérer par notre framework Protobox. Protobox est framework statefull conçu à base de Ansible. C\u0026rsquo;est un outil permettant de déployer des infrastructures à partir d\u0026rsquo;un schéma d\u0026rsquo;architecture défini dans l\u0026rsquo;inventaire principale.
2. Développement de Protobox #C\u0026rsquo;est un projet initié en 2019, aujourd\u0026rsquo;hui il a atteint une certaine maturité et sous peu de temps sera publique et en OpenSource.`,content:`1. Présentation #L\u0026rsquo;installation de l\u0026rsquo;infrastructure partant de l\u0026rsquo;adhésion des serveurs au déploiement des applications est gérer par notre framework Protobox. Protobox est framework statefull conçu à base de Ansible. C\u0026rsquo;est un outil permettant de déployer des infrastructures à partir d\u0026rsquo;un schéma d\u0026rsquo;architecture défini dans l\u0026rsquo;inventaire principale.
2. Développement de Protobox #C\u0026rsquo;est un projet initié en 2019, aujourd\u0026rsquo;hui il a atteint une certaine maturité et sous peu de temps sera publique et en OpenSource.
3. Usages #La 1e étape de la mise en place du schéma est la déclaration des unités logiques de l\u0026rsquo;infrastructure.
----------- | EXEMPLE | ----------- all: hosts: ... supervisor-2: arch: x86_64 os: ubuntu_22.04 model: nuc ansible_host: 192.168.1.7 roles: - supervisor networks_interfaces: box-network: dhcp: false network_manager: netplan ip: 192.168.1.7 ether: 88:ae:dd:xx:xx:xx gateways: - to: default via: 192.168.1.254 private-gateway: dhcp: false network_manager: netplan ip: 192.168.0.37 ether: 00:0e:c6:xx:xx:xx master-1: arch: x86_64 os: ubuntu_22.04 model: nuc ansible_host: 192.168.1.21 ip: 192.168.1.21 access_ip: 192.168.1.21 roles: - master networks_interfaces: box-network: dhcp: false network_manager: netplan ip: 192.168.1.21 ether: 1c:69:7a:xx:xx:xx gateways: - to: default via: 192.168.1.254 ... En plus de ses capacité à déployer des infrastructures, Protobox permet d\u0026rsquo;effectuer diverses opérations tel que:
La configuration réseau ----------- | EXEMPLE | ----------- supervisor-2: arch: x86_64 os: ubuntu_22.04 model: nuc ansible_host: 192.168.1.7 roles: - supervisor networks_interfaces: box-network: \u0026lt;------- box-deploy interface dhcp: false network_manager: netplan ip: 192.168.1.7 ether: 88:ae:dd:xx:xx:xx gateways: - to: default via: 192.168.1.254 private-gateway: \u0026lt;------- private-gateway interface dhcp: false network_manager: netplan ip: 192.168.0.37 ether: 00:0e:c6:xx:xx:xx Installation d\u0026rsquo;un LoadBalancer sous un FloatingIP ----------- | EXEMPLE | ----------- kube_master: vars: vips: - name: box-network master: master-1 interface: box-network virtual_router_id: 3 vip: \u0026quot;{{ loadbalancer_apiserver.address }}\u0026quot; hosts: master-1: box-network: priority: 200 state: MASTER master-2: box-network: priority: 100 state: BACKUP master-3: box-network: priority: 50 state: BACKUP Déploiement de Elasticsearch et Kibana dans le cluster Kubernetes ----------- | EXEMPLE | ----------- elasticsearch: vars: eck: version: 8.5.2 namespace: elastic-system app_templates: - name: local-storage - name: eck-crds - name: eck-operator - name: eck-es - name: kibana # - name: eck-local-storage hosts: webserver-1: role: kubectl master-1: role: master volumes: - name: elasticsearch-masters-1 size: 10Gi master-2: role: master volumes: - name: elasticsearch-masters-2 size: 10Gi master-3: role: master volumes: - name: elasticsearch-masters-3 size: 10Gi gateway-1: role: data volumes: - name: elasticsearch-data-1 size: 50Gi gateway-2: role: data volumes: - name: elasticsearch-data-2 size: 50Gi kibana: hosts: gateway-1: 4. Roles #Ce schema d\u0026rsquo;architecture est interprété et traité par diverses roles dont:
Roles Groupe Runtime factory/jenkins CI/CD systemd / docker-compose factory/gitlab CI/CD systemd / docker-compose DNSMASQ Network systemd network-setup Network netplan Docker Docker systemd docker-compose Docker Executable Docker Registry Docker systemd / docker-compose Gateway (HAProxy/Nginx) Network systemd / docker-compose KeepAlived Network systemd GlusterFS Volume systemd set-hostname OS linux monitoring/consul Monitoring systemd / docker-compose monitoring/prometheus Monitoring docker-compose monitoring/grafana Monitoring docker-compose monitoring/kiosk Monitoring wayland/xserver/chromium-headless monitoring/nodeexporter Monitoring systemd kubernetes/cluster-setup Kubernetes Kubernetes kubernetes/cni Kubernetes Kubernetes kubernetes/dashboard Kubernetes Kubernetes kubernetes/deploy-app Kubernetes Kubernetes kubernetes/kubectl-setup Kubernetes Kubernetes kubernetes/kubespray Kubernetes Kubernetes kubernetes/kustomize-install Kubernetes Kubernetes kubernetes/loadbalancer Kubernetes Kubernetes kubernetes/registry Kubernetes Kubernetes ldap Auth systemd / docker-compose `}),e.add({id:8,href:"/docs/monitoring/monitor/",title:"Monitor",description:"Prologue Doks.",content:`1. Présentation #Le monitoring est essentiel à toute infrastructure IT afin de detecter ou de prévoir d\u0026rsquo;éventuels incidents. Ce chapitre traite la surveillance matériel et logiciel de l\u0026rsquo;infrastructure et des outils utilisés pour assurer le bon fonctionnement du matériel.
2. Architecture #2.1. Inventaire #Le monitoring est assuré par le groupe de serveurs nat_gateway_pool comportant 2 serveurs intel nuc celeron
3. Networking #4. Applications #Prometheus Grafana Consul wpe-webkit-mir-kiosk (Headless Browser) Wayland 5. Installation #- name: Setup Hardware monitor hosts: supervisor-1 become: true user: supervisor gather_facts: false roles: - role: monitoring/init - role: monitoring/prometheus - role: monitoring/grafana - role: monitoring/kiosk - name: Setup software monitor hosts: nat_gateway_pool become: true user: supervisor gather_facts: false roles: - role: monitoring/consul `}),e.add({id:9,href:"/docs/presentation/",title:"Presentation",description:"Prologue Doks.",content:""}),e.add({id:10,href:"/docs/factory/ci/",title:"CI (Gitlab CI)",description:`1. Introduction #La CI est un des piliers de l\u0026rsquo;usine logiciel. C\u0026rsquo;est la phase qui construit pour la plupart des cas, le package d\u0026rsquo;une application. Les applications modernes sont souvent déployer sous forme de conteneurs. Du coup la construction et le stockage de leurs images s\u0026rsquo;effectuent pendant les étapes de la CI. Pour construire des images conteneurisées, il est impératif d\u0026rsquo;avoir un dépot git et un registry de conteneurs. Dans ce projet ces 2 éléments sont locaux, l\u0026rsquo;infrastructure dispose d\u0026rsquo;un serveur pour les 2 fonctionnalités.`,content:"1. Introduction #\rLa CI est un des piliers de l\u0026rsquo;usine logiciel. C\u0026rsquo;est la phase qui construit pour la plupart des cas, le package d\u0026rsquo;une application. Les applications modernes sont souvent déployer sous forme de conteneurs. Du coup la construction et le stockage de leurs images s\u0026rsquo;effectuent pendant les étapes de la CI. Pour construire des images conteneurisées, il est impératif d\u0026rsquo;avoir un dépot git et un registry de conteneurs. Dans ce projet ces 2 éléments sont locaux, l\u0026rsquo;infrastructure dispose d\u0026rsquo;un serveur pour les 2 fonctionnalités. Notre choix est porté sur Gitlab Server qui fédére les 2.\n2. Gitlab server #\r2.1. Installation #\rL\u0026rsquo;installation passe par un le role factory/gitlab qui selon le runtime spécifié lance Gitlab en mode container ou systemd\n------------------------------------------------------------- | path: playbooks-supervisor-pool/install.yml | ------------------------------------------------------------- - name: Setup supervisor host hosts: supervisor-1 become: true user: supervisor gather_facts: false roles: ... - {role: factory/gitlab, runtime: \u0026quot;systemd\u0026quot;, install: true} ... 2.2. Configuration #\r------------------------------------------------------------- | path: roles/factory/gitlab/templates/systemd/gitlab.rb.j2 | ------------------------------------------------------------- external_url 'http://git.protobox/gitlab' pages_external_url 'http://pages.protobox' gitlab_rails['initial_root_password'] = 'XXXXXXX' git_data_dirs({ \u0026quot;default\u0026quot; =\u0026gt; { \u0026quot;path\u0026quot; =\u0026gt; \u0026quot;/storage/shared/gitlab/data\u0026quot; } }) letsencrypt['enable'] = false nginx['listen_port'] = 80 3. Container Registry #\rLe constainer registry utilisé sur ce projet est celui embarqué dans Gitlab server (Désactivé par défaut). L\u0026rsquo;activation s\u0026rsquo;effectue au sein du fichier de configuration.\n------------------------------------------------------------- | path: roles/factory/gitlab/templates/systemd/gitlab.rb.j2 | ------------------------------------------------------------- registry_external_url 'https://registry.protobox' registry_nginx['listen_port'] = 443 registry_nginx['redirect_http_to_https'] = true registry_nginx['ssl_certificate'] = \u0026quot;{{ tls['registry']['crt'] }}\u0026quot; registry_nginx['ssl_certificate_key'] = \u0026quot;{{ tls['registry']['key'] }}\u0026quot; 4. Runners #\rLes runners sont les agents qui s\u0026rsquo;executent sur diverses machines pour exécuter des opérations télécommandées par Gitlab server. Pour l\u0026rsquo;installer sur un serveur, protobox utilise le role factory/gitlab-runners\nDéclaration (Exemple) #\r---------------------------------------------------- | path: playbooks-proxiserver-pool/nat-install.yml | ---------------------------------------------------- - name: Install nat-gateway-pool hosts: nat_gateway_pool become: true user: supervisor gather_facts: false roles: ... - {role: factory/gitlab-runners, runtime: \u0026quot;systemd\u0026quot;} ... Configuration générée (Exemple) #\r---------------------------------------- | path: /etc/gitlab-runner/config.toml | ---------------------------------------- concurrent = 1 check_interval = 0 shutdown_timeout = 0 [session_server] session_timeout = 1800 [[runners]] name = \u0026quot;gateway-3\u0026quot; url = \u0026quot;http://git.protobox/gitlab/\u0026quot; id = 6 token = \u0026quot;XXXXXXXXXXXXXX\u0026quot; token_obtained_at = 2023-02-20T21:22:03Z token_expires_at = 0001-01-01T00:00:00Z executor = \u0026quot;docker\u0026quot; [runners.docker] tls_verify = false image = \u0026quot;alpine\u0026quot; privileged = true disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\u0026quot;/var/run/docker.sock:/var/run/docker.sock\u0026quot;, \u0026quot;/cache\u0026quot;] shm_size = 0 network_mode = \u0026quot;host\u0026quot; 5. Kustomize #\rDans nos applications Kustomize est utilisé pour faire évoluer les versions des images qu\u0026rsquo;utilise Kubernetes. En effet cet outil nous permet également de personnaliser nos objets Kubernetes en fonction du type d\u0026rsquo;environnement (dev, test, homol, production etc)\n6. Pipelines #\r6.1. Configuration #\rPour chaque application il faudra créer le répertoire suivant dans lequel il y\u0026rsquo;a le fichier .gitlab-ci.yml pour définir la pipeline. Ceci est une convention fixée pour l\u0026rsquo;ensemble de nos applications\nfactory/CD/.gitlab-ci.yml Sur Gitlab server il faut également déclarer ce chemin.\nproject_name \u0026gt; Settings \u0026gt; CI/CD \u0026gt; CI/CD configuration file 6.2. Processus #\rLes processus sont divers et peuvent être relatifs à chaque projet. Cependant la plupart des applications utilisées passent pour la plupart des cas, par ces 3 phases\n6.2.1. Build #\rLa phase de build comporte plusieurs étapes. Elle sert principalement à construire l\u0026rsquo;image du container en question. Pendant cette phase, l\u0026rsquo;image est également poussée vers le registry.\nbuild: image: docker:stable stage: build services: - docker:dind before_script: - docker info - docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD $REGISTRY_URL script: - docker build -t $CI_REGISTRY_IMAGE/$CI_PROJECT_NAME:$CI_COMMIT_REF_NAME . - docker build -t $CI_REGISTRY_IMAGE/$CI_PROJECT_NAME:latest . - docker push $CI_REGISTRY_IMAGE/$CI_PROJECT_NAME:$CI_COMMIT_REF_NAME - docker push $CI_REGISTRY_IMAGE/$CI_PROJECT_NAME:latest 6.2.2. Update manifests #\rCet étape permet la montée de version de l\u0026rsquo;image utilisée par Kubernetes. En effet lors de cette phase les manifests utilisés sont mise à jour afin qu\u0026rsquo;ils soient déployés dans le cluster. Pour se faire, le pipeline va solliciter Kustomize.\nupdate_manifests: image: alpine:latest stage: update_manifests variables: GIT_STRATEGY: none retry: 2 before_script: - apk add --no-cache git curl bash openssh - curl -s \u0026quot;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026quot; | bash - mv kustomize /usr/local/bin/ - cd - git clone --single-branch --branch $CI_COMMIT_REF_NAME http://supervisor:$SUPERVISOR_ACCESS_TOKEN@git.protobox/gitlab/protobox1/docserver.git - git config --global user.name $REGISTRY_USERNAME - git config --global user.email $CI_USER_EMAIL script: - echo $CI_PROJECT_NAME - cd $CI_PROJECT_NAME - git fetch --prune - git checkout ${ARGO_TARGET_REVISION} - cd factory/CD/base - kustomize edit set image $CI_REGISTRY_IMAGE/$CI_PROJECT_NAME:$CI_COMMIT_REF_NAME - cat kustomization.yaml - cat deployment.yml - git add . - git commit -m '[skip ci] [ARGOCD] ${CI_COMMIT_MESSAGE}' - git push origin $ARGO_TARGET_REVISION 6.2.3. Synchronize ArgoCD #\rLa synchronisation consiste à déclencher indirectement le déploiement en le déléguant à ArgoCD. En effet sur nos applications ArgoCD se synchronisent manuellement (mode auto désactivé). De ce fait, après chaque mise à jour des manifests, le pipeline doit alerter ArgoCD afin que ce dernier puisse les récupérer.\nsynchronize_argo: image: alpine:latest stage: synchronize_argo variables: GIT_STRATEGY: none retry: 2 before_script: - apk add jq curl git delta - curl -sSf -L -o /usr/local/bin/argocd \u0026quot;https://github.com/argoproj/argo-cd/releases/download/v${ARGOCD_VERSION}/argocd-linux-amd64\u0026quot; - chmod +x /usr/local/bin/argocd - argocd login \u0026quot;${ARGO_SERVER_URL}\u0026quot; --insecure --username \u0026quot;${ARGO_USER_ID}\u0026quot; --password \u0026quot;${ARGO_USER_PASSWORD}\u0026quot; --plaintext script: - argocd app get argo-${CI_PROJECT_NAME} --refresh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 - argocd app sync argo-${CI_PROJECT_NAME} --assumeYes Variables #\r7. Mirroir #\r"}),e.add({id:11,href:"/docs/presentation/historique/",title:"Historique",description:`1. Version 1 #1.1 Infra #2. Version 2 #2.1 Infra #2.2 Architecture infra #3. Version 3 #4. Version 4 #4.1. Box #4.2. Architecture Infra #4.3. Deploiement applications #`,content:`1. Version 1 #1.1 Infra #2. Version 2 #2.1 Infra #2.2 Architecture infra #3. Version 3 #4. Version 4 #4.1. Box #4.2. Architecture Infra #4.3. Deploiement applications #`}),e.add({id:12,href:"/docs/authentification/kubedash/",title:"K8S Dashboard",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:13,href:"/docs/authentification/keycloak/",title:"Keycloak",description:`
1. Introduction #`,content:`
1. Introduction #`}),e.add({id:14,href:"/docs/authentification/kibana/",title:"Kibana",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:15,href:"/docs/kubernetes/master/",title:"Masters",description:`1. Présentation #Les masters sont répliqués pour garantir la haute disponibilité du cluster. Dans notre infrastructure, on dispose de 3 masters. Ces derniers sont le socle du Control Plane
2. Composants du Control Plane #Ref: kubernetes documentation
- kube-apiserver #Composant sur le master qui expose l\u0026rsquo;API Kubernetes. Il s\u0026rsquo;agit du front-end pour le plan de contrôle Kubernetes.
Il est conçu pour une mise à l\u0026rsquo;échelle horizontale, ce qui veut dire qu\u0026rsquo;il met à l\u0026rsquo;échelle en déployant des instances supplémentaires.`,content:`1. Présentation #Les masters sont répliqués pour garantir la haute disponibilité du cluster. Dans notre infrastructure, on dispose de 3 masters. Ces derniers sont le socle du Control Plane
2. Composants du Control Plane #Ref: kubernetes documentation
- kube-apiserver #Composant sur le master qui expose l\u0026rsquo;API Kubernetes. Il s\u0026rsquo;agit du front-end pour le plan de contrôle Kubernetes.
Il est conçu pour une mise à l\u0026rsquo;échelle horizontale, ce qui veut dire qu\u0026rsquo;il met à l\u0026rsquo;échelle en déployant des instances supplémentaires. Voir Construire des Clusters en Haute Disponibilité.
- etcd #Base de données clé-valeur consistante et hautement disponible utilisée comme mémoire de sauvegarde pour toutes les données du cluster.
Si votre cluster Kubernetes utilise etcd comme mémoire de sauvegarde, assurez-vous d\u0026rsquo;avoir un plan de back up pour ces données.
Vous pouvez trouver plus d\u0026rsquo;informations à propos d\u0026rsquo;etcd dans la documentation officielle.
- kube-scheduler #Composant sur le master qui surveille les pods nouvellement créés qui ne sont pas assignés à un nœud et sélectionne un nœud sur lequel ils vont s\u0026rsquo;exécuter.
Les facteurs pris en compte pour les décisions de planification (scheduling) comprennent les exigences individuelles et collectives en ressources, les contraintes matérielles/logicielles/politiques, les spécifications d\u0026rsquo;affinité et d\u0026rsquo;anti-affinité, la localité des données, les interférences entre charges de travail et les dates limites.
- kube-controller-manager #Composant du master qui exécute les contrôleurs.
Logiquement, chaque contrôleur est un processus à part mais, pour réduire la complexité, les contrôleurs sont tous compilés dans un seul binaire et s\u0026rsquo;exécutent dans un seul processus.
Ces contrôleurs incluent :
Node Controller : Responsable de détecter et apporter une réponse lorsqu\u0026rsquo;un nœud tombe en panne. Replication Controller : Responsable de maintenir le bon nombre de pods pour chaque objet ReplicationController dans le système. Endpoints Controller : Remplit les objets Endpoints (c\u0026rsquo;est-à-dire joint les Services et Pods). Service Account \u0026amp; Token Controllers : Créent des comptes par défaut et des jetons d\u0026rsquo;accès à l\u0026rsquo;API pour les nouveaux namespaces. cloud-controller-manager Le cloud-controller-manager exécute les contrôleurs qui interagissent avec les fournisseurs cloud sous-jacents. Le binaire du cloud-controller-manager est une fonctionnalité alpha introduite dans la version 1.6 de Kubernetes.
Le cloud-controller-manager exécute seulement les boucles spécifiques des fournisseurs cloud. Vous devez désactiver ces boucles de contrôleurs dans le kube-controller-manager. Vous pouvez désactiver les boucles de contrôleurs en définissant la valeur du flag \u0026ndash;cloud-provider à external lors du démarrage du kube-controller-manager.
Le cloud-controller-manager permet au code du fournisseur cloud et au code de Kubernetes d\u0026rsquo;évoluer indépendamment l\u0026rsquo;un de l\u0026rsquo;autre. Dans des versions antérieures, le code de base de Kubernetes dépendait du code spécifique du fournisseur cloud pour la fonctionnalité. Dans des versions ultérieures, le code spécifique des fournisseurs cloud devrait être maintenu par les fournisseurs cloud eux-mêmes et lié au cloud-controller-manager lors de l\u0026rsquo;exécution de Kubernetes.
Les contrôleurs suivants ont des dépendances vers des fournisseurs cloud :
Node Controller : Pour vérifier le fournisseur de cloud afin de déterminer si un nœud a été supprimé dans le cloud après avoir cessé de répondre Route Controller : Pour mettre en place des routes dans l\u0026rsquo;infrastructure cloud sous-jacente Service Controller : Pour créer, mettre à jour et supprimer les load balancers des fournisseurs cloud Volume Controller : Pour créer, attacher et monter des Volumes, et interagir avec le fournisseur cloud pour orchestrer les volumes.
3. Architecture #2. Roles #master-pool Ce groupe assure le bon fonctionnement du control plane supervisor Cette machine interagit avec l\u0026rsquo;apiserver pour commander le control plane. Les manifests destinés au déploiements dans ce host. 3. Networking #Interface:
Kubernetes CNI 5. Haute disponibilité #5.1. Principe de Quorum #Ref: Microsoft Learn
Le quorum est conçu pour empêcher les scénarios split-brain qui peuvent se produire lorsqu’il existe une partition au sein du réseau et que les sous-ensembles de nœuds ne peuvent pas communiquer entre eux. Cela peut amener les deux sous-ensembles de nœuds à tenter de s’approprier la charge de travail et à écrire sur le même disque, ce qui peut entraîner de nombreux problèmes. Toutefois, un tel scénario peut être évité grâce au concept de quorum du clustering de basculement, qui force l’exécution d’un seul de ces groupes de nœuds. De cette façon, un seul de ces groupes reste en ligne.
Le quorum détermine le nombre d’échecs que le cluster peut supporter tout en restant en ligne. Le quorum est conçu pour gérer les problèmes de communication entre les sous-ensembles de nœuds du cluster. Il empêche plusieurs serveurs d’héberger simultanément un groupe de ressources et d’écrire sur un même disque en même temps. Grâce à ce concept de quorum, le cluster force le service de cluster à s’arrêter dans l’un des sous-ensembles de nœuds de sorte qu’il n’y ait qu’un seul véritable propriétaire pour chaque groupe de ressources. Une fois que les nœuds qui ont été arrêtés peuvent de nouveau communiquer avec le groupe de nœuds principal, ils rejoignent automatiquement le cluster et démarrent leur service de cluster.
Recommandations relatives au quorum de clustera Si vous avez deux nœuds, il est obligatoire de disposer d’un témoin. Si vous disposez de trois ou quatre nœuds, le témoin est fortement recommandé. Si vous avez cinq nœuds ou plus, un témoin n’est pas nécessaire et ne fournit pas de résilience supplémentaire. Si vous avez accès à Internet, utilisez un témoin de cloud. Si vous êtes dans un environnement informatique qui comprend d’autres machines et partages de fichiers, utilisez un témoin de partage de fichiers.
5.2. Cluster ETCD #ETCD est la base de donnée par défaut de kubernetes. C\u0026rsquo;est une base de données key/value conçu pour stocker des paramètres de configuration.
ETCD fonctionne en cluster dont la haute disponibilité se base sur Quorum
La haute disponibilitê de kubernetes dépend non seulement du nombre de réplica du control plane mais aussi est fortement conditionnée par la haute disponibilité de ETCD
5.3 Rapport ETCD/Control Plane #Kubernetes élabore 2 types d\u0026rsquo;architecture de la relation ETCD/Control Plane:
3.3.1. Stacked Etcd Cluster #Sur ce type d\u0026rsquo;architecture chaque master embarque sa propre base de données. Ainsi le nombre de masters est strictement égal aux instance etcd. Dans ce cas de figure, le principe de quorum s\u0026rsquo;applique aux nombres de masters
Condition de disponibilité #Pour 3 noeuds la majorité de quorum est 2. En effet c\u0026rsquo;est le nombre au dessous duquel le cluster est hors service.
Table de quorum pour 3 noeuds #Noeud en service Noeud hors service Majorité Disponibilité 3 0 3 UP 2 1 2 UP 1 2 1 Down 3.3.2. External Etcd Cluster #L\u0026rsquo;externalisation des bases etcd permet d\u0026rsquo;extendre le quota de la majorité (le nombre d\u0026rsquo;instance en service) de quorum. Cette architecture a l\u0026rsquo;avantage d\u0026rsquo;agir sur la haute disponibilité sans ajouter des masters supplémentaires. Ce model ajoute une couche de sécurité en même temps la maintenance est plus souple.
Exemple: ETCD avec 3 instances
Exemple: ETCD avec 6 instances
Table de quorum pour 6 noeuds #Noeud en service Noeud hors service Majorité Disponibilité 6 0 6 UP 5 1 5 UP 4 2 4 UP 3 3 3 UP 2 4 2 Down 1 5 1 Down 3.3.3. architecture etcd de l\u0026rsquo;infrastructure #Notre choix est porté sur le stacked etcd cluster qui en terme de haute disponibilité respecte les standards.
4. Volumes #9. Bilan #`}),e.add({id:16,href:"/docs/iac/nodespyder/",title:"Nodespyder",description:`1. Présentation #Nodespyder est un logiciel de gestion d\u0026rsquo;infrastructure conçu dans le cadre de ce projet. Il est développé en Golang et se base principalement sur le protocole gRPC. Il comporte 2 systèmes dont:
spyder qui est une API Rest coté client et gRPC coté cluster spyvisor est un agent installé sur divers serveurs et interagit avec spyder 2. Fonctionnalités #Nodespyder est conçu pour gerer des infrastructures IT. Il fonctionne sur le principe de master/agent.`,content:`1. Présentation #Nodespyder est un logiciel de gestion d\u0026rsquo;infrastructure conçu dans le cadre de ce projet. Il est développé en Golang et se base principalement sur le protocole gRPC. Il comporte 2 systèmes dont:
spyder qui est une API Rest coté client et gRPC coté cluster spyvisor est un agent installé sur divers serveurs et interagit avec spyder 2. Fonctionnalités #Nodespyder est conçu pour gerer des infrastructures IT. Il fonctionne sur le principe de master/agent. L\u0026rsquo;API est installé sur une machine (pour le moment, clustering en mode réplica dans l\u0026rsquo;avenir) et des agent sur les machines avec lesquelles on interagit. Il permet de:
Collecter des données métriques (charge CPU, RAM, Disque etc) à partir des serveurs de l\u0026rsquo;infrastructure via l\u0026rsquo;agent Lancer des processus CI/CD (Pipeline) pour déployer des environnements dans l\u0026rsquo;univers cloud D\u0026rsquo;interagir avec des cluster Kubernetes pour executer des opérations telles que la consultation des logs, lister objets kubernetes etc. D\u0026rsquo;interagir avec des Container Registry pour consulter du contenu D\u0026rsquo;installer des outils sur des serveurs distants telque Nginx, HAProxy etc et d\u0026rsquo;assurer leur configuration Former des clusters applicatifs à partir d\u0026rsquo;interfaces web 3. Architecture #4. Network #`}),e.add({id:17,href:"/docs/authentification/prometheus/",title:"Prometheus",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:18,href:"/docs/r%C3%A9seaux/dhcp/",title:"Server DHCP",description:`1. Présentation #Le server DHCP est un des élements fondamentaux de l\u0026rsquo;infra. En effet l\u0026rsquo;ajout de machine passe par l\u0026rsquo;attribution d\u0026rsquo;IP et ainsi la machine nouvellement ajoutée est identifiée comme composant de la box.
Le framework protobox installe dnsmasq (server DNS/DHCP léger) sur toutes les machines classées supervisor-pool. Dnsmasq ne fonctionne pas en clustering même répliqué, sur chaque machine ce serveur fonctionne en indépendammant. En effet lors de la connexion en réseau d\u0026rsquo;une nouvelle machine ou celles qui ont un DHCP actif sur leurs paramètres réseau, elles lancent un signal pour une demande d\u0026rsquo;attribution IP alors le premier server DHCP recevant l\u0026rsquo;appel répond systematiquement.`,content:`1. Présentation #Le server DHCP est un des élements fondamentaux de l\u0026rsquo;infra. En effet l\u0026rsquo;ajout de machine passe par l\u0026rsquo;attribution d\u0026rsquo;IP et ainsi la machine nouvellement ajoutée est identifiée comme composant de la box.
Le framework protobox installe dnsmasq (server DNS/DHCP léger) sur toutes les machines classées supervisor-pool. Dnsmasq ne fonctionne pas en clustering même répliqué, sur chaque machine ce serveur fonctionne en indépendammant. En effet lors de la connexion en réseau d\u0026rsquo;une nouvelle machine ou celles qui ont un DHCP actif sur leurs paramètres réseau, elles lancent un signal pour une demande d\u0026rsquo;attribution IP alors le premier server DHCP recevant l\u0026rsquo;appel répond systematiquement.
2. DNSMASQ #Dnsmasq est un serveur léger conçu pour fournir les services DNS, DHCP, Bootstrap Protocol et TFTP pour un petit réseau, voire pour un poste de travail. Il permet d\u0026rsquo;offrir un service de nommage des machines du réseau interne non intégrées au service de nommage global (i.e. le service DNS d\u0026rsquo;Internet). Le service de nommage est associé au service d\u0026rsquo;adressage de telle manière que les machines dont le bail DHCP est fourni par Dnsmasq peuvent avoir automatiquement un nom DNS sur le réseau interne. Le logiciel offre un service DHCP statique ou dynamique.
2.1. Installation #Dnsmasq est développé sous forme de role ansible dans le framework protobox et utilisé dans le playbook-supervisor-install
- name: Setup gateway host hosts: supervisor_pool become: true user: supervisor gather_facts: false roles: - role: set-hostname - {role: network-setup} - role: etc-hosts - role: monitoring/node-exporter - role: docker - role: docker-compose - role: gateway-network-setup vars: installation: true - role: dnsmasq \u0026lt;------------------- vars: install: true Le role dnsmasq parcours les hosts définis dans l\u0026rsquo;inventory pour extraire les informations dont il a besoin pour référencer toutes les machines. Parmi ces informations on peut citer l\u0026rsquo;adresse MAC, l\u0026rsquo;IP et l\u0026rsquo;interface réseau à l\u0026rsquo;écoute
Template utilisé par le role pour générer la configuration dnsmasq interface={{ box_network }} listen-address=0.0.0.0 server=8.8.8.8 server=8.8.4.4 #no-resolv #domain-needed # Don’t forward short names bogus-priv expand-hosts dhcp-range={{ range_start }},{{ range_end }},{{ refresh_delay }} {% for host in groups['all'] %} {% for interface in hostvars[host]['networks_interfaces'] %} {% if hostvars[host]['networks_interfaces'][interface]['ether'] is defined and interface == box_network %} dhcp-host={{ hostvars[host]['networks_interfaces'][interface]['ether'] }},{{ host }},{{ hostvars[host]['networks_interfaces'][interface]['ip'] }} {% endif %} {% endfor %} {% endfor %} 2.2. Activation du DHCP Client pour une demande automatique d\u0026rsquo;IP #host-1: arch: x86_64 os: ubuntu_22.04 model: nuc ansible_host: 192.168.X.XX ip: 192.168.X.XX access_ip: 192.168.X.XX roles: - gateway networks_interfaces: box-network: dhcp: true \u0026lt;----------------- network_manager: netplan ip: 192.168.X.XX ether: 88:ae:dd:XX:XX:XX `}),e.add({id:19,href:"/docs/monitoring/hardware/",title:"Surveillance matériel",description:`1. Présentation #La surveillance matériel est géré par un des serveurs de nat_gateway_pool. Diverses données sont recueillies sur chaque serveur du cluster (CPU, RAM, Disques, Réseaux etc) et centralisées sur le serveur de monitoring. Ces données sont utilisés par Prometheus pour stocker, traiter, servir ces données et Grafana pour la visualisation.
2. Architecture #3. Node-exporter #Nodeexporter est un agent qui permet d\u0026rsquo;exposer les métriques d\u0026rsquo;une machine. C\u0026rsquo;est une API qui effectue des requêtes sur le host puis s\u0026rsquo;interfaçe sous forme d\u0026rsquo;API Rest afin de synchroniser les données avec d\u0026rsquo;autres système telque Prometheus`,content:`1. Présentation #La surveillance matériel est géré par un des serveurs de nat_gateway_pool. Diverses données sont recueillies sur chaque serveur du cluster (CPU, RAM, Disques, Réseaux etc) et centralisées sur le serveur de monitoring. Ces données sont utilisés par Prometheus pour stocker, traiter, servir ces données et Grafana pour la visualisation.
2. Architecture #3. Node-exporter #Nodeexporter est un agent qui permet d\u0026rsquo;exposer les métriques d\u0026rsquo;une machine. C\u0026rsquo;est une API qui effectue des requêtes sur le host puis s\u0026rsquo;interfaçe sous forme d\u0026rsquo;API Rest afin de synchroniser les données avec d\u0026rsquo;autres système telque Prometheus
3.1. Installlation #Tous les serveurs de l\u0026rsquo;infrastructure embarque Nodeexporter. L\u0026rsquo;installation s\u0026rsquo;effectue pendant la phase d\u0026rsquo;initialisation d\u0026rsquo;une machine dans l\u0026rsquo;infrastructure.
------------------------------------------------------- | path: playbook-host-install.yml | ------------------------------------------------------- - name: Install host hosts: host-x become: true user: supervisor roles: - role: monitoring/node-exporter 4. Prometheus #Prometheus est un logiciel libre de surveillance informatique et générateur d\u0026rsquo;alertes. Il enregistre des métriques en temps réel dans une base de données de séries temporelles (avec une capacité d\u0026rsquo;acquisition élevée) en se basant sur le contenu de point d\u0026rsquo;entrée exposé à l\u0026rsquo;aide du protocole HTTP. Ces métriques peuvent ensuite être interrogées à l\u0026rsquo;aide d\u0026rsquo;un langage de requête simple (PromQL) et peuvent également servir à générer des alertes. Le projet est écrit en Go et est disponible sous licence Apache 2. Le code source est disponible sur GitHub2, et est un projet maintenu par la Cloud Native Computing Foundation à côté d\u0026rsquo;autres projets comme Kubernetes et Envoy3. (Ref: Wikipédia)
4.1. Installation #------------------------------------------------------- | path: playbook-supervisor-install.yml | ------------------------------------------------------- - name: Setup supervisor host hosts: supervisor-2 become: true user: supervisor gather_facts: false roles: - role: monitoring/init - role: monitoring/prometheus \u0026lt;------- - role: monitoring/grafana - role: monitoring/kiosk --------------------------------------------------------- | path: inventorie/protobox/webservers_pool/routes.yml | --------------------------------------------------------- prometheus: webserver: - location: /prometheus/ params: proxy_pass: values: \u0026quot;http://stream_prometheus/prometheus/\u0026quot; stream: name: stream_prometheus port: 9090 group: nat_gateway_pool 5. Grafana #Grafana est un logiciel libre sous licence GNU Affero General Public License Version 32 (anciennement sous licence Apache 2.0 avant avril 2021) qui permet la visualisation de données. Il permet de réaliser des tableaux de bord et des graphiques depuis plusieurs sources dont des bases de données temporelles comme Graphite (en), InfluxDB et OpenTSDB3. (Ref: Wikipédia)
5.3. Installation #------------------------------------------------------- | path: playbook-supervisor-install.yml | ------------------------------------------------------- - name: Setup supervisor host hosts: supervisor-2 become: true user: supervisor gather_facts: false roles: - role: monitoring/init - role: monitoring/prometheus - role: monitoring/grafana \u0026lt;------- - role: monitoring/kiosk --------------------------------------------------------- | path: inventorie/protobox/webservers_pool/routes.yml | --------------------------------------------------------- grafana: webserver: - location: /grafana/ params: rewrite: values: ^/grafana/(.*) /\$1 break proxy_set_header: values: Host \$http_host proxy_pass: values: \u0026quot;http://stream_grafana/grafana\u0026quot; - location: /grafana/api/live/ params: rewrite: values: ^/grafana/(.*) /\$1 break proxy_http_version: values: 1.1 proxy_set_header: type: repeat values: Upgrade: \$http_upgrade Connection: \$connection_upgrade Host: \$http_host proxy_pass: values: \u0026quot;http://stream_grafana/grafana\u0026quot; stream: name: stream_grafana port: 3000 group: nat_gateway_pool `}),e.add({id:20,href:"/docs/iac/",title:"Iac",description:"Prologue Doks.",content:`
`}),e.add({id:21,href:"/docs/factory/cd/",title:"CD (GitOps)",description:`1. Présentation #Notre CD (Continuous Delivery) repose sur le principe de GitOps. En effet GitOps est une pratique qui consiste à conceptualiser et planifier les opérations liées à l\u0026rsquo;évolution d\u0026rsquo;un environnement applicatif sur la base d\u0026rsquo;un ou plusieurs repository Git. Notre CD est géré par ArgoCD qui est un outil réputé pour son efficacité.
2. ArgoCD #2.1. Installation #Dans le logiciel protobox, le role factory/argocd permet de déployer ArgoCD dans le cluster Kubernetes.`,content:`1. Présentation #Notre CD (Continuous Delivery) repose sur le principe de GitOps. En effet GitOps est une pratique qui consiste à conceptualiser et planifier les opérations liées à l\u0026rsquo;évolution d\u0026rsquo;un environnement applicatif sur la base d\u0026rsquo;un ou plusieurs repository Git. Notre CD est géré par ArgoCD qui est un outil réputé pour son efficacité.
2. ArgoCD #2.1. Installation #Dans le logiciel protobox, le role factory/argocd permet de déployer ArgoCD dans le cluster Kubernetes.
- name: Setup supervisor host hosts: supervisor-1 become: true user: supervisor gather_facts: false roles: ... - role: factory/argocd ... 2.2. Configuration d\u0026rsquo;application #Pour instaurer le mécanisme de déploiement d\u0026rsquo;une application dans le système ArgoCD, il ajouter créer l\u0026rsquo;objet Kubernetes de type Application. La déclaration s\u0026rsquo;effectue dans le repertoire suivant pour chaque projet:
factory \u0026gt; CD \u0026gt; argo \u0026gt; application.yml Exemple d\u0026rsquo;application #apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: argo-docserver namespace: argocd spec: project: default source: repoURL: http://192.168.1.7/gitlab/protobox1/docserver.git targetRevision: CD path: factory/CD/base destination: server: https://kubernetes.default.svc syncPolicy: syncOptions: - CreateNamespace=true 2.3. Amélioration à apporter #En effet un problème de résolution de DNS se pose actuellement au sein du conteneur argocd-server n\u0026rsquo;ayant pas la capacité de trouver les nom de domaine des serveurs privés gitlab. Dans l\u0026rsquo;exemple ci dessus le lien du répository git comporte une ip et non le nom de domaine correspondante. argo-server lance un message
dial tcp: lookup git.protobox on 169.254.25.10:53: no such host Pour remédier à ce problème 2 pistes s\u0026rsquo;offrent à nous (pour le moment en tout cas):
1e piste. Combiner CoreDNS à consul ou DNSMASQ pour la résolution DNS de nos repositories git.
------------------ | CoreDNS/Consul | ------------------ apiVersion: v1 kind: ConfigMap metadata: labels: addonmanager.kubernetes.io/mode: EnsureExists name: coredns namespace: kube-system data: Corefile: | .:53 { \u0026lt;Existing CoreDNS definition\u0026gt; } + consul { + errors + cache 30 + forward . \u0026lt;consul-address\u0026gt; + } ------------------ | CoreDNS/DNSMASQ | ------------------ apiVersion: v1 kind: ConfigMap metadata: labels: addonmanager.kubernetes.io/mode: EnsureExists name: coredns namespace: kube-system data: Corefile: | .:53 { \u0026lt;Existing CoreDNS definition\u0026gt; } + dnsmasq { + errors + cache 30 + forward . \u0026lt;-dnsmasq-address\u0026gt; + } 2e piste Cette piste consiste à forcer /etc/hosts de la machine hôte dans le conteneur argocd-server
`}),e.add({id:22,href:"/docs/authentification/grafana/",title:"Grafana",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:23,href:"/docs/presentation/objectif/",title:"Objectif",description:`Ce projet est conçu pour des usages professionnelles notamment des services applicatifs. Ses usages sont diversifiés partant d\u0026rsquo;un environnement de dev à la production. Dans une infrastructure IT, les performances, la sécurité et la haute disponibilité sont des facteurs qui définissent la qualité de service.
1. Sécurité #La sécurité d\u0026rsquo;un système informatique repose sur des principes, conventions et bonnes pratiques. Dans un infrastructure IT les unités sont interconnectées et se transmettent probablement des informations.`,content:`Ce projet est conçu pour des usages professionnelles notamment des services applicatifs. Ses usages sont diversifiés partant d\u0026rsquo;un environnement de dev à la production. Dans une infrastructure IT, les performances, la sécurité et la haute disponibilité sont des facteurs qui définissent la qualité de service.
1. Sécurité #La sécurité d\u0026rsquo;un système informatique repose sur des principes, conventions et bonnes pratiques. Dans un infrastructure IT les unités sont interconnectées et se transmettent probablement des informations. La plupart des attaques se greffent sur ces échanges pour atteindre leurs cibles. En sécurité informatique, la maitrise en réseau est fondamentale.
1.1. Réseau #Dans ce projet, on peut voir différents réseaux. Ces cloisonnements se justifient par la sécurité. En effet l\u0026rsquo;infrastructure communique principalement par un réseau central et est non accessible directement depuis l\u0026rsquo;extérieur. Cette topologie réseau protège fortement le système des attaques.
Chapitre Architecture réseau (Suite) 1.2. Certificats et TLS #La communication chiffrée est un des piliers de la sécurité. En effet le chiffrement dans le contexte IT consiste à coder les données échangées entre 2 entités. Ce mécanisme rend difficile l\u0026rsquo;exploitation de ces données. Pour cela, nous priviligions les échanges TLS
Chapitre Proxiserver (Suite) 1.3. Firewalls #Les firewalls représentent plus ou moins la police aux frontières. En effet les firewalls controlent et autorisent les entrées et sorties. Ils sont actifs au sein de chaque serveur et attribut l\u0026rsquo;accessibilité réseau en fonction des réglès qui lui sont données.
2. Haute disponibilité #La haute disponibilité est un facteur essentiel dans l\u0026rsquo;IT notamment dans des environnement de production. La haute disponibilité trouve sa définition dans la capacité à assurer la disponibilité permanente de service. Elle obéit à des principes et conventions.
2.1. Point of failure #Dans un cycle de traitement d\u0026rsquo;une requête, cette dernière passe par plusieurs points de traitement (webserver, proxy, serveurs etc\u0026hellip;). En cas de blocage sur un de ces points, la rêquete peut échouer. Si on raisonne en terme de service un point de défaillance, plus connu sous le nom de \u0026ldquo;Point of Failure\u0026rdquo; est un point dont son dysfonctionnement entraine l\u0026rsquo;arrêt total du ou des services. Dans un système informatique, les points de défaillance sont antagonistes à la haute disponibilité. Plus il existe des points de défaillances, plus la haute disponibilité baisse. Ce sont des points impératifs à supprimer. La réplication des instances est une des moyens de les contourner. Dans ce projet, on peut scinder l\u0026rsquo;infrastructure en 2:
Le cluster kubernetes qui permet d\u0026rsquo;exposer des services (site web, API Rest, dashboard databases etc..) à l\u0026rsquo;exterieur. Ce type de service ne doit admettre aucun point de défaillance d\u0026rsquo;ou la nécessité de les répliquer.
Le système de gestion du cluster qui ne sert qu\u0026rsquo;à gerer l\u0026rsquo;infrastructure. Parmi les entités de ce système on peut citer les services de monitoring (Grafana, Prometheus, Consul etc) et des systèmes d\u0026rsquo;usine logiciel (Gitlab, Container Registry, ArgoCD). En effet le dysfonctionnement de ces services n\u0026rsquo;est pas une fatalité pour l\u0026rsquo;utilisateur exterieur. Leur panne n\u0026rsquo;impacte que le personnel (administrateur, développeur etc) qui gère l\u0026rsquo;infra. Du coup leur haute disponibilité peut être reléguer au second plan.
2.2. Réplication physique #La réplication physique des serveurs consiste à dupliquer une machine physique par le biais d\u0026rsquo;une Floating
Chapitre FloatingIP (Suite) 2.3. Réplication d\u0026rsquo;applications #Une bonne partie des applications modernes fonctionnent en mode clustering, c\u0026rsquo;est à dire un ensemble d\u0026rsquo;instances dupliquées et interconnectées pour fournir (relativement) les mêmes services. D\u0026rsquo;autres applications sont réplicables hors cluster.
2.3. Réplication de masters kubernetes #Dans notre infrastructure, on dispose de 3 masters pour assurer la haute disponibilité de notre cluster kubernetes. Ces 3 masters jouent le même role.
Chapitre Kubernetes Masters (Suite) 3. Performance #La performance des systèmes IT est relative à plusieurs facteurs. Les performances dépendent fortement des ressources attribuées à chaque machine. Nos machines sont calibrées en fonction de l\u0026rsquo;usage. Pour les bases de données et moteurs de recherche, on est dans une grosse configuration et quant aux autres machines on a des serveurs de faibles consommation d\u0026rsquo;énergie et de ressources un peu limites. Il n\u0026rsquo;y a pas que les ressources qui font les performances. Il y\u0026rsquo;a l\u0026rsquo;architecture des SI et d\u0026rsquo;autres éléments qui conditionnent les performances.
`}),e.add({id:24,href:"/docs/kubernetes/services/",title:"Services",description:`1. Types de service #Les services kubernetes servent principalement à exposer les déploiements kubernetes. On distingue 3 types de services dont:
ClusterIP Ce type de service est utilisé pour les communications internes entre les différents pods NodeIP Les NodeIP contrairement aux ClusterIP permettemt d\u0026rsquo;externaliser les services. Ils exposent leur port à tous les nodes du cluster LoadBalancer Les loadBalancers sont plus ou moins similaires aux NodeIP mais ne sont fonctionnels qu\u0026rsquo;à la présence d\u0026rsquo;un controleur complémentaire qui leur attribut un IP.`,content:`1. Types de service #Les services kubernetes servent principalement à exposer les déploiements kubernetes. On distingue 3 types de services dont:
ClusterIP Ce type de service est utilisé pour les communications internes entre les différents pods NodeIP Les NodeIP contrairement aux ClusterIP permettemt d\u0026rsquo;externaliser les services. Ils exposent leur port à tous les nodes du cluster LoadBalancer Les loadBalancers sont plus ou moins similaires aux NodeIP mais ne sont fonctionnels qu\u0026rsquo;à la présence d\u0026rsquo;un controleur complémentaire qui leur attribut un IP. En effet les loadbalancers font une requête auprès d\u0026rsquo;un controleur dédié à l\u0026rsquo;attribution d\u0026rsquo;un PI. 2. Ingress #Un ingress est un élément de kubernetes qui joue le role de serveur web et route le trafic vers d\u0026rsquo;autres services afin en se basant sur des endpoints. Les ingress sont gérés par un ou des controleur appelé controleur d\u0026rsquo;ingress qui maintiennent ce service.
Cas d\u0026rsquo;usage: Lorsqu\u0026rsquo;on expose directement le cluster vers l\u0026rsquo;exterieur c\u0026rsquo;est à dire que les requêtes passent directement par point d\u0026rsquo;entrée du cluster, ce type de service est adapté et oriente les demandes.
Inconvénient: La réplication de l\u0026rsquo;ingress n\u0026rsquo;est pas possible (à ma connaissance). En effet les controleurs d\u0026rsquo;ingress sont réplicables et lorsque le controleur conteneur de l\u0026rsquo;ingress est hors service l\u0026rsquo;instance se perd et le basculement de l\u0026rsquo;ingress ne s\u0026rsquo;effectue pas. C\u0026rsquo;est un système qui crée un point de défaillance au cluster. Dans le cadre de la haute disponilité, aucun point de défaillance n\u0026rsquo;est pas accepté.
2. NodeIP #Les NodeIP ont l\u0026rsquo;avantage de répliquer nativement un service. En effet il expose un service sur toutes les machines du cluster.
Cas d\u0026rsquo;usage: Notre cluster actuel utilise ce type de service. En effet l\u0026rsquo;infrastructure intégre au front (avant le réseau du cluster) un serveur web et un reverse-proxy dans le même réseau privée que les noeuds (gateway-pool) kubernetes, il serait préférable de ne pas réproduire un autre serveur web dans le cluster. Puisqu\u0026rsquo;un service est répliqué sur toutes les machines de kubernetes, le nginx au front va appliquer un loadbalance sur ces dernières. En terme de sécurité, la communication s\u0026rsquo;effectue dans un réseau cloisonné, nos noeuds ne sont pas accessible depuis l\u0026rsquo;exterieur.
3. Architecture #Le cluster kubernetes est isolé des serveurs web (Nginx) et communique à travers un reverse-proxy (HAProxy). Cela offre une couche de sécurité importante.
4. Haute disponibilité #7. Externalisation d\u0026rsquo;un service #Pour exposer un service sur internet, suivre les étapes suivantes:
1. Définir et applique le manifest kubernetes du service
Exemple:
--- apiVersion: v1 kind: Service metadata: labels: app: pgadmin name: pgadmin namespace: postgresql spec: ports: - name: padmin-port nodePort: 30165 port: 80 targetPort: 80 selector: app: pgadmin type: NodePort --- 2. Déclarer le service dans HAProxy via le groupvars kube_gateway_pool
Exemple:
---------------------------------------------------------- | path: inventories/protobox/kube_gateway_pool/proxy.yml | ---------------------------------------------------------- pgadmin4: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.pgadmin4.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.pgadmin4.port }}\u0026quot; balance: roundrobin options: - httplog 3. Configuration du server web Nginx via le groupvars webservers_pool
Exemple:
---------------------------------------------------------- | path: inventories/protobox/webservers_pool/routes.yml | ---------------------------------------------------------- pgadmin4: webserver: - location: /v-admin/ params: proxy_set_header: type: repeat values: Host: \$http_host X-Real-IP: \$remote_addr X-Forwarded-For: \$proxy_add_x_forwarded_for proxy_pass: values: http://stream_vadmin stream: name: stream_vadmin port: 30164 group: kube_gateway_pool `}),e.add({id:25,href:"/docs/monitoring/services/",title:"Surveillance applicative",description:`1. Introduction #`,content:`1. Introduction #`}),e.add({id:26,href:"/docs/infrastructure/webservers/",title:"Webservers Pool",description:`1. Présentation #Webservers pool est le point d\u0026rsquo;entrée public (internet) de l\u0026rsquo;infra. Ce sont des serveurs répliqués ayant les mêmes fonctions. Ces serveurs s\u0026rsquo;interfacent sur 2 réseaux dont le réseau du fournisseur d\u0026rsquo;accès à internet et celui du réseau cluster-gateway. En effet ces machines n\u0026rsquo;ont pas l\u0026rsquo;accès direct au réseau principal de la box proto (box-network) pour des raisons de sécutité. Ces serveurs internet recoivent les rêquêtes en provenance d\u0026rsquo;internet puis les transmettent aux serveurs de proxy (gateway-pool)`,content:`1. Présentation #Webservers pool est le point d\u0026rsquo;entrée public (internet) de l\u0026rsquo;infra. Ce sont des serveurs répliqués ayant les mêmes fonctions. Ces serveurs s\u0026rsquo;interfacent sur 2 réseaux dont le réseau du fournisseur d\u0026rsquo;accès à internet et celui du réseau cluster-gateway. En effet ces machines n\u0026rsquo;ont pas l\u0026rsquo;accès direct au réseau principal de la box proto (box-network) pour des raisons de sécutité. Ces serveurs internet recoivent les rêquêtes en provenance d\u0026rsquo;internet puis les transmettent aux serveurs de proxy (gateway-pool)
Position dans l\u0026rsquo;infra
2. Roles #Les webserveurs ont principalement 3 fonctions:
serveur d\u0026rsquo;application avec Nginx gestion et renouvellement du certificat utilisé avec Certbot et Let\u0026rsquo;s Encrypt routage vers internet avec Netplan 3. Networking #Ce groupe s\u0026rsquo;interface sur 2 réseaux:
public-network pour router le trafic sur internet gateway-network pour accéder aux services du cluster 4. Volumes #Le grope webserver dispose d\u0026rsquo;un volume partagé permettant de partager des fichiers de configuration telque:
certificats configurations nginx 5. Haute disponibilité #La haute disponibilité est essentielle à ce niveau et tout point de défaillance est à exclure. Pour cela 2 floatingIP sont créees:
public-floatingIP: permet de répliquer l\u0026rsquo;IP utilisée pour les requêtes entrantes private-floatingIP: gérer la réplication de l\u0026rsquo;IP interne (cluster-gateway) afin de router avec haute disponibilité le traffic sortant 6. Sécurité #La sécurité de ce groupe est basé principalement sur:
iptables
port entrant: 80 443 certificats
7. Applications #Nginx comme server applicatif Certbot pour la gestion des certificats KeepAlived pour les floatingIP GlusterFS pour clusteriser les volumes 8. Installation #L\u0026rsquo;installation des ces serveurs sont automatisées par le framework. Il suffit de les affecter dans les groups vars de webservers_pool
webservers_pool: vars: output_interface: public-gateway input_interface: cluster-gateway # Declaration of floatingIPs vips: # internet-floatingIP public-gateway: virtual_router_id: 1 vip: 192.168.0.33 # internal-floatingIP cluster-gateway: virtual_router_id: 2 vip: 192.168.2.254 hosts: webserver-1: public-gateway: # used for configuring KeepAlived priority: 200 state: MASTER cluster-gateway: # used for configuring KeepAlived priority: 100 state: BACKUP webserver-2: public-gateway: # used for configuring KeepAlived priority: 100 state: BACKUP cluster-gateway: # used for configuring KeepAlived priority: 200 state: MASTER 9. Bilan #Application Status Installation et mise en service DONE Nginx DONE Certbot DONE Keepalived DONE GlusterFS TODO `}),e.add({id:27,href:"/docs/infrastructure/",title:"Infrastructure",description:"Prologue Doks.",content:`
`}),e.add({id:28,href:"/docs/r%C3%A9seaux/proxyserve/",title:"Proxiserver",description:"Prologue Doks.",content:`1. Présentation #Proxiserver est la combinaison des webservers et gateways qui joue un role fondamental dans le système. Les webservers n\u0026rsquo;ont pas accès direct aux réseaux et services. Pour exposer les applications sur internet ou autres réseaux externes, les serveurs web doivent interroger les gateways.
2. Architecture et Networking #Les webservers et gateways communiquent dans un réseau privé particulier et sont les seuls à y accéder. C\u0026rsquo;est choix conditionné par la gestion du certificat qu\u0026rsquo;on étudiera dans le prochain chapitre.
3. Exemple de configuration du routage #prometheus: gateway: mode: http address: \u0026quot;*\u0026quot; port: 9090 destination: addresses: - name: supervisor-1 address: 192.168.1.6 port: 9090 balance: roundrobin options: - httplog webserver: - location: /prometheus/ params: proxy_pass: values: \u0026quot;http://{{ gateway_external_vip }}:9090/prometheus/\u0026quot; grafana: gateway: mode: http address: \u0026quot;*\u0026quot; port: 3000 destination: addresses: - name: supervisor-1 address: 192.168.1.6 port: 3000 balance: roundrobin options: - httplog Configuration HAProxy générée par le framework protobox
4. Certificats #4.1. Patterns d\u0026rsquo;ecryptage #4.1.1. Pattern EDGE #Ce mode convient aux connexions dans un réseau interne hautement sécurisé où le proxy inverse maintient une connexion sécurisée (HTTP sur TLS) avec les clients tout en communiquant avec l\u0026rsquo;application en mode HTTP sans TLS. Ce mode de fontionnement permet d\u0026rsquo;avoir un TLS commun à un ensemble d\u0026rsquo;applications
4.1.2. Pattern PASSTHROUGH #Ce mode convient aux applications configurées pour fournir leurs propres certificats. Pour éviter d\u0026rsquo;encrypter 2 fois la communication au détriment des performances, le reverse proxy utilise une communication HTTP pour faire un BYPASS
4.1.3. Pattern RE-ENCRYPT #Ce mode renforce considérablement la sécurité en utilisant 2 encryptages différents. Pour cela 2 certificats sont installés:
Un certicat front est installé sur le reverse proxy pour encrypter la communication avec le client Un certificat back qui établit une connection TLS entre le reverse et le client 5. Choix Pattern pour le proxiserver #5.1. Communication HTTP/HTTPS #5.2. Interfaçage réseau #`}),e.add({id:29,href:"/docs/r%C3%A9seaux/vip/",title:"FloatingIP",description:`1. Présentation #Une FloatingIP connu également sous le nom de \u0026ldquo;VIP (Virtual IP)\u0026rdquo; est une adresse static publique qui permet d\u0026rsquo;exposer des services tel que des hosts, des LoadBalancer etc sous une même ip. En effet une FloatingIP est principalement affectée à une machine (MASTER) tant que cette dernière est en service et bascule sur une autre machine (BACKUP) en cas de panne du MASTER.
2. KeepAlived #KeepAlived est un logiciel de routage permettant des installations simples et robustes pour l\u0026rsquo;équilibrage de charge qui repose sur le module du noyau Linux IPVS et le protocole VRRP.`,content:`1. Présentation #Une FloatingIP connu également sous le nom de \u0026ldquo;VIP (Virtual IP)\u0026rdquo; est une adresse static publique qui permet d\u0026rsquo;exposer des services tel que des hosts, des LoadBalancer etc sous une même ip. En effet une FloatingIP est principalement affectée à une machine (MASTER) tant que cette dernière est en service et bascule sur une autre machine (BACKUP) en cas de panne du MASTER.
2. KeepAlived #KeepAlived est un logiciel de routage permettant des installations simples et robustes pour l\u0026rsquo;équilibrage de charge qui repose sur le module du noyau Linux IPVS et le protocole VRRP.
2.1. Internet Protocol Virtual Server (IPVS) #IPVS est un module du noyau Linux utlisé pour faire des opérations réseaux de bas niveau. En effet ce module est beaucoup utilisé et peu connu à la fois, les iptables s\u0026rsquo;interfacent avec pour manoeuvrer les firewalls. Dans le contexte de haute disponibilité, ce module permet l\u0026rsquo;équilibrage de charge du noyau Linux en agissant sur la couche 4 du modèle OSI
2.2. Virtual Router Redundancy Protocol (VRRP) #VRRP est un protocole standard dont le but est d\u0026rsquo;augmenter la disponibilité de la passerelle par défaut des hôtes d\u0026rsquo;un même réseau. Le principe est de définir la passerelle par défaut pour les hôtes du réseau comme étant une adresse IP virtuelle référençant un groupe de routeurs.
2.3. Fonctionnement #Lors de la configuration, 2 paramètres sont attendus pour chaque serveur:
Le State qui spécifie le type c\u0026rsquo;est à dire MASTER ou BACKUP La Priorité qui détermine le BACKUP qui prendra la relève en cas de panne du MASTER Tous les instances en service #Quand toutes les instances sont en service, le MASTER par défaut porte le FloatingIP
Le MASTER hors service #En cas de panne du MASTER, le FloatingIP bascule sur le BACKUP possèdant la priorité la plus élevée
3. Installation #Avec le framework protobox, le pool du FloatingIP s\u0026rsquo;effectue par déclaration du group dans l\u0026rsquo;inventaire principale.
---------------------------------------- | path: inventories/protobox/main.yml | ---------------------------------------- servers_pool: vars: vips: network-1: virtual_router_id: 1 vip: 192.168.1.10 network-2: virtual_router_id: 2 vip: 192.168.2.10 hosts: host-1: network-1: priority: 200 state: MASTER network-2: priority: 100 state: BACKUP host-2: network-1: priority: 100 state: BACKUP network-2: priority: 200 state: MASTER Resultat #Cette déclaration ci-dessus crée 2 FloatingIP sur 2 réseaux différents.
host-1 est le MASTER sur le réseau network-1 et le BACKUP sur network-2 host-2 est le MASTER sur le réseau network-2 et le BACKUP sur network-1 Configuration généré sur le host-1 ----------------------------------------- | path: /etc/keepalived/keepalived.conf | | host: host-1 | ----------------------------------------- vrrp_instance network-1-host-1 { state MASTER interface network-1 virtual_router_id 1 priority 200 advert_int 1 authentication { auth_type PASS auth_pass 1988 } virtual_ipaddress { 192.168.1.10/24 } } vrrp_instance network-2-host-1 { state BACKUP interface network-2 virtual_router_id 2 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1988 } virtual_ipaddress { 192.168.2.10/24 } } Configuration généré sur le host-2 ----------------------------------------- | path: /etc/keepalived/keepalived.conf | | host: host-2 | ----------------------------------------- vrrp_instance network-1-host-2 { state BACKUP interface network-1 virtual_router_id 1 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1988 } virtual_ipaddress { 192.168.1.10/24 } } vrrp_instance network-2-host-2 { state MASTER interface network-2 virtual_router_id 2 priority 200 advert_int 1 authentication { auth_type PASS auth_pass 1988 } virtual_ipaddress { 192.168.2.10/24 } } `}),e.add({id:30,href:"/docs/infrastructure/gateways/",title:"Gateways Pool",description:`1. Présentation #Gateway pool est un ensemble de hosts répliqués qui servent essentiellement de proxy entre gateway-network et box-network mais aussi permettent d\u0026rsquo;exposer les NodeIP du cluster Kubernetes. Ces serveurs sont des noeuds Kubernetes auxquelles les services exposent les pods
2. Roles #Sur chaque serveur est répliqué HAProxy qui route le traffic. Ce tyoe d\u0026rsquo;architecture permet de protèger le réseau principal de la box. C\u0026rsquo;est la police aux frontières entre box-network et cluster-gateway.`,content:`1. Présentation #Gateway pool est un ensemble de hosts répliqués qui servent essentiellement de proxy entre gateway-network et box-network mais aussi permettent d\u0026rsquo;exposer les NodeIP du cluster Kubernetes. Ces serveurs sont des noeuds Kubernetes auxquelles les services exposent les pods
2. Roles #Sur chaque serveur est répliqué HAProxy qui route le traffic. Ce tyoe d\u0026rsquo;architecture permet de protèger le réseau principal de la box. C\u0026rsquo;est la police aux frontières entre box-network et cluster-gateway. Un service ou API n\u0026rsquo;est utilisé par les webserveurs seulement s\u0026rsquo;il est déclaré sur le gateway
Gateway Pool est scindé en 2 groupe:
3. kube-gateway-pool #3.1. Roles #Ce groupe est un composant du cluster Kubernetes permettant à ce dernier d\u0026rsquo;exposer ses services. En effet l\u0026rsquo;exposition des services du cluster aux webserver passe par ce groupe de machines et uniquement.
3.2. Networking #Interfaces réseaux: gateway-network box-network 3.3. Volumes #3.4. Installation #Pour former le groupe, il faut le déclarer dans l\u0026rsquo;inventory principale
kube_gateway_pool: hosts: gateway-1: gateway-2: gateway-3: 3.5. Routage #Les services Kubernetes ne peuvent franchir les gateways que s\u0026rsquo;ils sont déclarés dans :
Déclaration:
inventory principale app: arogcd: port: XXXXX pgadmin4: port: XXXXX dash: port: XXXXX keycloak: port: XXXXX kibana: port: XXXXX v-admin: port: XXXXX Declaration dans group vars (Magics variables) proxies: target: group: kube_gateway_pool routes: pgadmin4: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.pgadmin4.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.pgadmin4.port }}\u0026quot; balance: roundrobin options: - httplog argocd: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.arogcd.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.arogcd.port }}\u0026quot; balance: roundrobin options: - httplog dash: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.dash.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.dash.port }}\u0026quot; balance: roundrobin options: - httplog keycloak: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.keycloak.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.keycloak.port }}\u0026quot; balance: roundrobin options: - httplog kibana: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.kibana.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.kibana.port }}\u0026quot; balance: roundrobin options: - httplog v-admin: mode: http address: \u0026quot;*\u0026quot; port: \u0026quot;{{ app.v-admin.port }}\u0026quot; destination: group: name: kube_gateway_pool port: \u0026quot;{{ app.v-admin.port }}\u0026quot; balance: roundrobin options: - httplog 3.6. Haute disponibilité #La haute disponibilité est assurée par la réplication. On dispose de 3 machines disposant plus ou moins de la même configuration
3.7. Sécurité #iptables: géré par kubernetes 3.8. Applications #HAProxy pour router le trafic GlusterFS pour clusteriser les volumes Kubernetes services pour exposer les déploiements Containerd comme containers runtime 4. nat-gateway-pool #3.1. Roles #Ce groupe sert principalement:
à acheminer les requêtes vers internet (Iptables, Netplan) à servir les applications internes du boitier précisément les applications utilisées uniquement pour le fonctionnement du boitier à assurer le monitoring de l\u0026rsquo;infrastructure (Prometheus, Grafana) et des services (Consul). En effet c\u0026rsquo;est le seul groupe qui a accès aux divers réseaux, ce qui lui donne la légitimité de surveiller l\u0026rsquo;ensemble des unités logiques de linfrastructure 3.2. Networking #Interfaces réseaux: gateway-network box-network 3.3. Volumes #3.4. Installation #nat_gateway_pool: vars: output_interface: gateway-network input_interface: box-network vips: box-network: virtual_router_id: 5 vip: 192.168.1.254 hosts: gateway-4: box-network: priority: 200 state: MASTER gateway-5: box-network: priority: 100 state: BACKUP 3.5. Routage #Chaque machine de nat-gateway-pool a principalement comme role de router le trafic vers les services fonctionnels du boitier, c\u0026rsquo;est à dire les applications servant à manager ou surveiller le bon fonctionnement du cluster. Parmi les application on peut citer Grafana, Prometheus, Consult, Gitlab
3.6. Haute disponibilité #3.7. Sécurité #iptables (à compléter) 3.8. Applications #Grafana Prometheus Consul Gitlab `}),e.add({id:31,href:"/docs/r%C3%A9seaux/",title:"Réseaux",description:"Réseaux",content:`
`}),e.add({id:32,href:"/docs/infrastructure/databases/",title:"Databases Pool",description:`1. Présentation #Ce groupe de servers est dédié aux bases de données et possède des ressources matérielles élevées. Les serveurs bases de bases consomment sont relativement gourmands en RAM et processeur d\u0026rsquo;où l\u0026rsquo;équipement en Intel i5 et 16Go de RAM voir 32Go
2. Roles #Les machines de ce groupe sont principalement des serveurs de bases de données. Ils sont répliqués pour assurer la haute disponibilité.
Emplacement dans le cluster`,content:`1. Présentation #Ce groupe de servers est dédié aux bases de données et possède des ressources matérielles élevées. Les serveurs bases de bases consomment sont relativement gourmands en RAM et processeur d\u0026rsquo;où l\u0026rsquo;équipement en Intel i5 et 16Go de RAM voir 32Go
2. Roles #Les machines de ce groupe sont principalement des serveurs de bases de données. Ils sont répliqués pour assurer la haute disponibilité.
Emplacement dans le cluster
3. networking #Les serveurs de bases de données sont gérés par Kubernetes et sont accéssibles que de puis box-network
4. Volumes #Ces machines sont rattachées au cluster de volumes central du boitier, ainsi les backups sont partagés et préservés en réplication sur d\u0026rsquo;autre machines
5. Haute disponibilité #6. Sécurité #iptables (à définir) 7. Applications #Postgresql Redis (à installer) 8. Installation #Ces hosts ne sont pas encore installés du fait du nombre de machines disponibles mais selon mis en place progressivement. Pour le moment postgresql est installé sur les machines gateway le temps d\u0026rsquo;achat de ces machines
9. Bilan #`}),e.add({id:33,href:"/docs/kubernetes/",title:"Kubernetes",description:"Prologue Doks.",content:`
`}),e.add({id:34,href:"/docs/infrastructure/search-engine/",title:"Search Engine Pool",description:`1. Présentation #Ce groupe de servers est dédié aux bases de données et possède des ressources matérielles élevées. Les serveurs search engines consomment sont relativement gourmands en RAM et processeur d\u0026rsquo;où l\u0026rsquo;équipement en Intel i5 et 16Go de RAM voir 32Go Elasticsearch est principalement utilisé comme noeuds de données et execute les recherches sur ses données pendant que la gestion du cluster Elasticsearch est assurée par les masters pool`,content:`1. Présentation #Ce groupe de servers est dédié aux bases de données et possède des ressources matérielles élevées. Les serveurs search engines consomment sont relativement gourmands en RAM et processeur d\u0026rsquo;où l\u0026rsquo;équipement en Intel i5 et 16Go de RAM voir 32Go Elasticsearch est principalement utilisé comme noeuds de données et execute les recherches sur ses données pendant que la gestion du cluster Elasticsearch est assurée par les masters pool
2. Roles #Elasticsearch deploie plusieurs noeuds et classe ces derniers en groupe selon leurs fonctions. Parmi on peut citer:
Master Nodes: Gestion du cluster
Les noeuds masters d\u0026rsquo;Elasticsearch cohabitent en sidecar avec les masters Kubernetes
Data Nodes: Gestion des données
3. networking #Les serveurs search engines sont gérés par Kubernetes et sont accéssibles que de puis box-network
Emplacement dans le cluster
4. Volumes #Ce groupe dispose est connecté au volume partagé du cluster pour stocker leurs backups
5. Haute disponibilité #6. Sécurité #iptables gérés par kubernetes Istio à installer pour les communications TLS internes 7. Applications #Elasticsearch Redis à installer 8. Installation #L\u0026rsquo;installation est automatisée par le framework. La configuration se base sur du déclaratif (Inventory). Les volumes et noeuds sont crées en fonction du paramètrage sur chaque hosts du groups vars
elasticsearch: vars: eck: version: 8.5.2 namespace: elastic-system app_templates: - name: local-storage - name: eck-crds - name: eck-operator - name: eck-es - name: kibana # - name: eck-local-storage hosts: webserver-1: role: kubectl master-1: role: master volumes: - name: elasticsearch-masters-1 size: 10Gi master-2: role: master volumes: - name: elasticsearch-masters-2 size: 10Gi master-3: role: master volumes: - name: elasticsearch-masters-3 size: 10Gi gateway-1: role: data volumes: - name: elasticsearch-data-1 size: 50Gi gateway-2: role: data volumes: - name: elasticsearch-data-2 size: 50Gi \$ play playbook-se-install `}),e.add({id:35,href:"/docs/factory/",title:"Factory (CI/CD)",description:"Factory",content:`
`}),e.add({id:36,href:"/docs/infrastructure/masters/",title:"Masters Pool",description:`1. Présentation #Dans l\u0026rsquo;infra, on dispose de 3 masters qui assurent le clustering de Kubernetes ainsi que les masters applicatifs des autres programmes. Cette cohabitation est nommée Sidecar
2. Roles #Emplacement dans le cluster
2.1. Sidecar #Les applications qui fonctionnent en clustering seront configurées pour placer leurs masters applicatifs au sein de ces serveurs masters d\u0026rsquo;où la notion de sidecar
3. Networking #3.1. Interfaces #box-network 4.`,content:`1. Présentation #Dans l\u0026rsquo;infra, on dispose de 3 masters qui assurent le clustering de Kubernetes ainsi que les masters applicatifs des autres programmes. Cette cohabitation est nommée Sidecar
2. Roles #Emplacement dans le cluster
2.1. Sidecar #Les applications qui fonctionnent en clustering seront configurées pour placer leurs masters applicatifs au sein de ces serveurs masters d\u0026rsquo;où la notion de sidecar
3. Networking #3.1. Interfaces #box-network 4. Volumes #5. Haute disponibilité #La haute disponibilité est mieux décrite dans la section Kubernetes
6. Sécurité #iptables gérés par kubernetes Istio à installer pour les communications TLS internes 7. Applications #Elasticsearch masters Kubernetes masters Kubeadm Kubectl Kustomize KeepAlived HAProxy GlusterFS Calico 8. L\u0026rsquo;installation #L\u0026rsquo;installation est décrite en détails dans la section Kubernetes
`}),e.add({id:37,href:"/docs/monitoring/",title:"Monitoring",description:"Prologue Doks.",content:`
`}),e.add({id:38,href:"/docs/infrastructure/supervisor/",title:"Supervisor Pool",description:`1. Présentation #Dans l\u0026rsquo;infra, on dispose de 3 masters qui assurent le clustering de Kubernetes ainsi que les masters applicatifs des autres programmes. Cette cohabitation est nommée Sidecar
2. Roles #Supervisor joue un grand role dans la gestion et le bon fonctionnement de cluster.
Administration du cluster Kubernetes en interagissant avec le Control plane Stockage des backups via le réseau (GlusterFS) Commande le versionning et la CI avec Gitlab Server Management de l\u0026rsquo;infrastructure avec le logiciel protobox (Ansible) Joue le role de bastion pour l\u0026rsquo;accès ssh Externalisation de git sur un réseau privé destinée aux développeurs Bastion pour vscode 3.`,content:`1. Présentation #Dans l\u0026rsquo;infra, on dispose de 3 masters qui assurent le clustering de Kubernetes ainsi que les masters applicatifs des autres programmes. Cette cohabitation est nommée Sidecar
2. Roles #Supervisor joue un grand role dans la gestion et le bon fonctionnement de cluster.
Administration du cluster Kubernetes en interagissant avec le Control plane Stockage des backups via le réseau (GlusterFS) Commande le versionning et la CI avec Gitlab Server Management de l\u0026rsquo;infrastructure avec le logiciel protobox (Ansible) Joue le role de bastion pour l\u0026rsquo;accès ssh Externalisation de git sur un réseau privé destinée aux développeurs Bastion pour vscode 3. Networking #3.1 Interfaces #box-network 4. Volumes #5. Applications #Gitlab Server Gitlab Containers Registry Docker Kubectl Kustomize GlusterFS Ansible 6. Installation #- name: Setup supervisor host hosts: supervisor-2 become: true user: supervisor gather_facts: false roles: - role: set-hostname - {role: network-setup} - {role: gateway, iptables: true, nginx_enabled: false, haproxy_enabled: false, iptables: false} - role: etc-hosts - role: monitoring/node-exporter - role: docker - role: docker-compose - role: dnsmasq vars: install: true vars: installation: false - {role: ldap, install: false} - role: factory/init - {role: factory/gitlab, runtime: \u0026quot;systemd\u0026quot;, install: true} - role: kubernetes/kubectl-setup vars: distant_host: true 7. Bilan #`}),e.add({id:39,href:"/docs/authentification/",title:"Authentification",description:"Prologue Doks.",content:""}),e.add({id:40,href:"/docs/",title:"Docs",description:"Docs Doks.",content:`
`}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()